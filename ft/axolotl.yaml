base_model: Qwen/Qwen2-7B-Instruct
load_in_8bit: false
load_in_4bit: true
bnb_4bit_compute_dtype: bfloat16

chat_template: chatml

datasets:
  - path: ft/sft.jsonl
    type: json
    field_messages: [instruction, input, output]
    conversation: false

output_dir: ft/out
eval_interval: 200
save_steps: 200
logging_steps: 20
num_epochs: 2
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 2.0e-4
lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules: [q_proj, k_proj, v_proj, o_proj]

gradient_checkpointing: true
warmup_ratio: 0.03
lr_scheduler: cosine
optimizer: adamw_torch